{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2, torch, math, time, sys\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "sys.path.append('..')\n",
    "from load_data.load_dataset import *\n",
    "from util.utils import *\n",
    "from data_evaluate import *\n",
    "from pre.img_enhancement import *\n",
    "\n",
    "import model.Mobilefacenet_SE as Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'se_0728'\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lr, num_epochs = 0.01, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = r'palm_vein\\para'\n",
    "img_dir = '../dataset/enhance_img'\n",
    "test_dir = '../dataset/enhance_img2'\n",
    "\n",
    "train_data = get_data(img_dir)\n",
    "test_data = get_data(test_dir)\n",
    "train_iter = get_data_iter(train_data, batch_size)\n",
    "test_iter = get_data_iter(test_data, batch_size)\n",
    "\n",
    "num_classes = train_data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 2.50M\n"
     ]
    }
   ],
   "source": [
    "name = 'se_0728'\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_num = 256\n",
    "net_para = {'fp16':False,\n",
    "            'num_features':feature_num}\n",
    "class_para = {'in_features':feature_num,\n",
    "              'out_features':train_data.classes}\n",
    "\n",
    "model = Model.Full_Model(False, 256).to(device)\n",
    "\n",
    "# 参数量\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD([\n",
    "    {'params': model.parameters()}\n",
    "], lr=0.1, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpus = False\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/50 ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Linear_Softmax.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m optimizer_ft\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     32\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/palm_vein/train/../model/Mobilefacenet_SE.py:130\u001b[0m, in \u001b[0;36mFull_Model.forward\u001b[0;34m(self, img, y, classifier)\u001b[0m\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mArcMargin(x, y)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear_Softmax.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# 不知道为啥第一次开的时候巨小\n",
    "# 最好开一次，break，再开一次\n",
    "if RESUME:\n",
    "    ckpt = torch.load(RESUME)\n",
    "    model.load_state_dict(ckpt['net_state_dict'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "test_acc = 0.0\n",
    "best_acc = 0.0\n",
    "best_epoch = 0\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if epoch>0 and epoch%20==0:\n",
    "         lr /= 5\n",
    "    # train model\n",
    "    metric = Accumulator(3)\n",
    "    print('Train Epoch: {}/{} ...'.format(epoch+1, num_epochs))\n",
    "    model.train()\n",
    "\n",
    "    train_total_loss = 0.0\n",
    "    total = 0\n",
    "    since = time.time()\n",
    "    for idx, data in enumerate(train_iter):\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        batch_size = img.shape[0]\n",
    "        optimizer_ft.zero_grad()\n",
    "\n",
    "        output = model(img, label, 'soft')\n",
    "\n",
    "        total_loss = criterion(output, label)\n",
    "        total_loss.backward()\n",
    "        optimizer_ft.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "                metric.add(total_loss.item() * batch_size, \n",
    "                           accuracy(output, label), batch_size)\n",
    "\n",
    "    train_loss = metric[0] / (metric[2]+1e-4)\n",
    "    train_acc = metric[1] / (metric[2]+1e-4)\n",
    "    time_elapsed = time.time() - since\n",
    "    if epoch>0 and epoch % 10 ==0:\n",
    "        test_acc = evaluate_accuracy(test_iter, model)\n",
    "\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, epoch time %.1f sec'\n",
    "              % (epoch + 1, train_loss, train_acc, \n",
    "                 test_acc, \n",
    "                 time_elapsed))\n",
    "'''\n",
    "    # test model on lfw\n",
    "    if epoch % TEST_FREQ == 0:\n",
    "        net.eval()\n",
    "        featureLs = None\n",
    "        featureRs = None\n",
    "        print('Test Epoch: {} ...'.format(epoch))\n",
    "        for data in testloader:\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].cuda()\n",
    "            res = [net(d).data.cpu().numpy() for d in data]\n",
    "            featureL = np.concatenate((res[0], res[1]), 1)\n",
    "            featureR = np.concatenate((res[2], res[3]), 1)\n",
    "            if featureLs is None:\n",
    "                featureLs = featureL\n",
    "            else:\n",
    "                featureLs = np.concatenate((featureLs, featureL), 0)\n",
    "            if featureRs is None:\n",
    "                featureRs = featureR\n",
    "            else:\n",
    "                featureRs = np.concatenate((featureRs, featureR), 0)\n",
    "\n",
    "        result = {'fl': featureLs, 'fr': featureRs, 'fold': folds, 'flag': flags}\n",
    "        # save tmp_result\n",
    "        scipy.io.savemat('./result/tmp_result.mat', result)\n",
    "        accs = evaluation_10_fold('./result/tmp_result.mat')\n",
    "        _print('    ave: {:.4f}'.format(np.mean(accs) * 100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_iter, model, soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(save_dir, f'model_{name}.pt'))\n",
    "torch.save(final, os.path.join(save_dir, f'soft_{name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_model = torch.load(f'./para/model_{name}.pt',map_location=torch.device('cuda'))\n",
    "read_soft = torch.load(f'./para/soft_{name}.pt',map_location=torch.device('cuda'))\n",
    "evaluate_accuracy(test_iter, read_model, read_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(data_iter):\n",
    "    data_tensor = []\n",
    "    read_model.eval()# 评估模式, 这会关闭dropout\n",
    "    with torch.no_grad():\n",
    "        for X_b, y_b in data_iter:\n",
    "            xx = read_model(X_b.cuda()).cpu()\n",
    "            for X,y in zip(xx, y_b):\n",
    "                data_tensor.append([X, y.item()])\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = get_feature(train_iter)\n",
    "test_feature = get_feature(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_feature, os.path.join(save_dir, f'train_feature_{name}.pt'))\n",
    "torch.save(test_feature, os.path.join(save_dir, f'test_feature_{name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.8888,  2.9698, -0.3431, -2.3977, -0.8264, -1.2213, -1.7418, -1.8570,\n",
       "         -1.3909, -0.4047, -3.6878,  2.3282,  0.3908,  2.6455, -1.2222,  2.0165,\n",
       "          3.6979, -2.7159,  2.5104,  0.4924, -0.0176,  0.0311,  1.8223, -0.6642,\n",
       "          0.3250, -2.3430, -2.0666,  0.9897,  2.6380, -2.0554, -3.3723,  1.1724,\n",
       "         -1.8458,  1.6718,  1.0929, -1.0691, -2.7810, -1.1431, -1.1305, -2.2232,\n",
       "          2.7189, -0.1913,  2.3936,  1.4009, -3.6227,  3.8984,  0.1198,  1.5051,\n",
       "         -3.1359, -2.7623,  0.4558, -0.8613,  1.1408, -0.2466,  1.2250,  2.6903,\n",
       "         -1.8833, -3.5605,  0.7728,  0.4142,  0.7286,  0.4714, -3.3772, -0.6448,\n",
       "         -0.0655, -2.9439, -0.0662,  2.8782,  0.1644, -1.2064,  1.2021, -0.0257,\n",
       "         -2.7823, -1.9061,  0.1910, -0.4191, -0.4766, -2.2711,  0.9827, -1.9899,\n",
       "         -3.4095,  0.6065, -2.9991,  0.4260, -3.4609,  1.2900, -2.2389, -0.9299,\n",
       "          3.6921,  1.5202,  2.4377, -2.3685, -2.8346, -4.1692,  0.3946,  2.0910,\n",
       "          0.7904,  0.2392, -0.1305, -1.8995,  1.7379,  1.7453, -4.7772,  1.4235,\n",
       "         -0.1928,  0.6575,  0.1646,  2.4411, -0.7654, -1.0027,  2.0152, -2.2814,\n",
       "          3.4054,  3.5066,  0.2703, -2.9536,  3.7577,  0.9322,  1.0273, -3.1107,\n",
       "          1.7162,  0.7113, -2.2703,  1.0206, -1.8102, -0.8331,  1.5462,  2.4100,\n",
       "         -0.3484,  0.8418, -4.3624,  2.5359,  1.9406,  1.3277,  1.9988, -0.5909,\n",
       "         -0.3645, -0.2342, -1.4257,  3.7815, -2.8989,  0.7060,  1.4917,  3.5534,\n",
       "          2.5942, -0.8989,  3.8838,  0.2252, -1.1349,  2.7256,  0.9470,  3.2199,\n",
       "         -0.4753,  1.5082,  1.8162,  0.4403, -1.8694, -2.1288, -4.5802, -2.5617,\n",
       "          2.6812, -2.5261,  0.2106, -1.0596,  0.8916, -3.6586,  3.3966,  3.9755,\n",
       "         -4.2652, -3.5733,  0.2935, -0.7857, -1.3034,  4.3122,  1.8493, -3.0359,\n",
       "         -3.4429, -3.5821,  0.0667,  1.5202,  2.1262,  0.4967,  0.0833,  1.1069,\n",
       "         -1.5034, -0.2283,  0.4596, -0.8749,  1.0228,  3.6603, -1.0339, -1.5539,\n",
       "         -0.1954, -2.0552, -0.4037, -2.1370,  0.0954, -0.3807, -0.5688,  2.5687,\n",
       "         -1.2368,  2.8297, -0.9237, -1.2178, -0.0905, -1.2572,  0.5983, -0.5381,\n",
       "          1.4073, -0.8658, -1.4768, -5.6168,  0.9425, -1.5754,  1.5125, -3.0504,\n",
       "         -2.5401,  0.8922, -1.6729,  2.3591,  2.6850,  2.5651,  1.5218, -3.7362,\n",
       "         -1.3107, -1.0643,  1.1735, -0.9126, -0.6898,  2.9027, -4.5064,  3.2523,\n",
       "          2.3888,  0.3561, -3.9565,  3.8975,  0.2323, -0.3131, -1.2143,  3.2466,\n",
       "         -2.4520,  0.1409, -1.6379,  0.6160, -2.1828, -0.9968, -0.9767,  3.1511,\n",
       "          3.5415,  2.6889, -0.4494,  2.6758,  2.1609,  1.1437,  2.9922, -0.6987]),\n",
       " 52]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算二分类下的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "test_lab = {}\n",
    "for idx, fea in enumerate(test_feature):\n",
    "    output_dict[idx] = fea[0]\n",
    "    test_lab[idx] = fea[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x):\n",
    "    m,n = x.shape\n",
    "    G = np.dot(x.T, x)\n",
    "    H = np.tile(np.diag(G), (n,1))\n",
    "    return H + H.T - 2*G\n",
    "\n",
    "data = [np.array(x[0]) for x in test_feature]\n",
    "data = np.array(data).T\n",
    "corr = dist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "congeneric_flag = []\n",
    "dist_list = []\n",
    "for i in range(len(data[0])):\n",
    "    for j in range(i + 1, len(data[0])):\n",
    "        dist_list.append(np.sqrt(corr[i][j]))\n",
    "        congeneric_flag.append(test_lab[i] == test_lab[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold集合，排序算roc\n",
    "th_list = np.round(np.array(dist_list),1)\n",
    "th_list = np.sort(th_list)\n",
    "th_list =np.unique(th_list)\n",
    "\n",
    "y_score = np.asarray(dist_list)\n",
    "# 协同对\n",
    "y_true = np.asarray(congeneric_flag)\n",
    "# 非协同对标记\n",
    "y_false = (y_true == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_th = th_list.shape[0]\n",
    "th = th_list\n",
    "far = [0]*n_th\n",
    "frr = [0]*n_th\n",
    "\n",
    "best_acc = best_th = 0\n",
    "EER = 100\n",
    "\n",
    "idx_l, idx_r = 0, n_th\n",
    "idx = (idx_r + idx_l)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_th):\n",
    "    y_test = (y_score < th[i])\n",
    "    acc = np.mean((y_test == y_true).astype(int))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_th = th\n",
    "    \n",
    "    test_acc = (y_test == y_true)\n",
    "    test_like = (test_acc * y_true)\n",
    "    test_unlike = (test_acc * y_false)\n",
    "\n",
    "    frr[i] = 1. - np.sum(test_like)/np.sum(y_true)\n",
    "    far[i] = 1. - np.sum(test_unlike)/np.sum(y_false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27a44c48c40>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(1-np.array(far),1-np.array(frr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer_idx = np.argmin(abs(np.array(far)-np.array(frr)))\n",
    "eer = 1-(far[eer_idx]+frr[eer_idx])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860945619241946"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空显存\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
